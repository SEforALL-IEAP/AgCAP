{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "These are the functions needed in order to run the extraction notebook. **Please do not edit this file as it will break the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd   # Note that you require geopandas version >= 0.7 that incluse clip see here for installation (https://gis.stackexchange.com/questions/360127/geopandas-0-6-1-installed-instead-of-0-7-0-in-conda-windows-10#)\n",
    "import os\n",
    "import fiona\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "import rasterio.fill\n",
    "import shapely\n",
    "from shapely.geometry import shape, mapping\n",
    "import pyproj\n",
    "import json\n",
    "#from earthpy import clip    clip has been deprecated to geopandas\n",
    "#import earthpy.spatial as es\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import gdal\n",
    "import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import scipy.spatial\n",
    "from scipy.spatial import Voronoi\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting admin 1 boundary name to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_admin1_name(clusters, admin_col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the admin 1 boundaries')\n",
    "    admin_1 = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    \n",
    "    # Apply spatial join \n",
    "    cluster_support_2 = gpd.sjoin(clusters_support, admin_1[[\"geometry\", admin_col_name]], op='intersects').drop(['index_right'], axis=1)\n",
    "    group_by_id = cluster_support_2.groupby([\"id\"]).sum().reset_index()\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', admin_col_name]], on='id', how = 'left')\n",
    "    clusters.rename(columns = {admin_col_name:'Admin_1'}, inplace = True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_admin1_name_bulk(clusters, file_name, admin_col_name, crs):\n",
    "    # Import layer\n",
    "    #messagebox.showinfo('OnSSET', 'Select the admin 1 boundaries')\n",
    "    admin_1 = gpd.read_file(file_name)\n",
    "    \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    \n",
    "    # Apply spatial join \n",
    "    cluster_support_2 = gpd.sjoin(clusters_support, admin_1[[\"geometry\", admin_col_name]], op='intersects').drop(['index_right'], axis=1)\n",
    "    group_by_id = cluster_support_2.groupby([\"id\"]).sum().reset_index()\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', admin_col_name]], on='id', how = 'left')\n",
    "    clusters.rename(columns = {admin_col_name:'Admin_1'}, inplace = True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_admin_name(clusters, admin, admin_col_name):\n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    clusters_support_centroid = clusters_support.copy()\n",
    "    clusters_support_centroid.geometry = clusters_support_centroid.centroid\n",
    "    \n",
    "    # Apply spatial join \n",
    "    clusters_support_centroid_2 = gpd.sjoin(clusters_support_centroid, admin[[\"geometry\", admin_col_name]], op='intersects').drop(['index_right'], axis=1)\n",
    "    group_by_id = clusters_support_centroid_2.groupby([\"id\"]).sum().reset_index()\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', admin_col_name]], on='id', how = 'left')\n",
    "    #clusters.rename(columns = {admin_col_name:'Admin_name'}, inplace = True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting IDP & Refugee camps characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_IDPs_RefugeeCamps_status(clusters, col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the layer of IDP')\n",
    "    idp_gdf = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    \n",
    "    # Apply spatial join and group by cluster \"id\"\n",
    "    pointsInPolygon = gpd.sjoin(idp_gdf, clusters_support, how=\"inner\", op='intersects')\n",
    "    pointsInPolygon[col_name]=1\n",
    "    group_by_id = pointsInPolygon.groupby([\"id\", col_name]).sum().reset_index().drop(\"index_right\", axis=1)\n",
    "    \n",
    "    # Merge back to clusters\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', col_name]], on='id', how = 'left')\n",
    "    \n",
    "    clusters[col_name] = np.where(clusters[col_name] > 0, 1, 0)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting No of building per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_buildings_in_clusters(clusters, col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the layer of building footprints')\n",
    "    gdf = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    #Converting polygon buildings to points\n",
    "    gdf_centroids = gpd.GeoDataFrame(gdf,\n",
    "                                     crs=\"EPSG:4326\",\n",
    "                                     geometry=[Point(xy) for xy in zip(gdf.centroid.x, gdf.centroid.y)])\n",
    "    \n",
    "    # Reverting clusters to original crs \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    #clusters_support.id = clusters_support.id.astype(int)\n",
    "    \n",
    "    # Apply spatial join and group by cluster \"id\"\n",
    "    pointsInPolygon = gpd.sjoin(gdf_centroids, clusters_support, how=\"inner\", op='intersects')\n",
    "    pointsInPolygon[col_name]=1\n",
    "    group_by_id = pointsInPolygon.groupby([\"id\"]).sum().reset_index().drop(\"index_right\", axis=1)\n",
    "    \n",
    "    # Merge back to clusters\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', col_name]], on='id', how = 'left')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    clusters[col_name] = clusters[col_name].fillna(0)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting No of water points per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_waterpoints_in_clusters(clusters, col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the layer of water points')\n",
    "    gdf = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    # Reverting clusters to original crs \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    \n",
    "    # Apply spatial join and group by cluster \"id\"\n",
    "    pointsInPolygon = gpd.sjoin(gdf, clusters_support, how=\"inner\", op='intersects')\n",
    "    pointsInPolygon[col_name]=1\n",
    "    group_by_id = pointsInPolygon.groupby([\"id\"]).sum().reset_index().drop(\"index_right\", axis=1)\n",
    "    \n",
    "    # Merge back to clusters\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', col_name]], on='id', how = 'left')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    clusters[col_name] = clusters[col_name].fillna(0)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_raster(name, method, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_raster_bulk(file_name, name, method, clusters):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(file_name)\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "## Processing Categorical/Discrete Rasters\n",
    "def processing_raster_cat(path, raster, prefix, polys):\n",
    "    \"\"\"\n",
    "    This function calculates stats for categorical rasters and attributes them to the given vector features. \n",
    "    \n",
    "    INPUT: \n",
    "    path: the directory where the raster layer is stored \n",
    "    raster: the name and extention of the raster layer \n",
    "    prefix: string used as prefix when assigning features to the vectors\n",
    "    clusters: the vector layer containing the clusters\n",
    "    \n",
    "    OUTPUT:\n",
    "    geojson file of the vector features including the new attributes\n",
    "    \"\"\"    \n",
    "    raster=rasterio.open(path + '\\\\' + raster)\n",
    "    \n",
    "    polys = zonal_stats(\n",
    "        polys,\n",
    "        raster.name,\n",
    "        categorical=True,\n",
    "        prefix=prefix, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(\"{} processing completed at\".format(prefix), datetime.datetime.now())\n",
    "    return polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Land cover area estimator\n",
    "def calc_Crop_sqkm(df, col_list):\n",
    "    \"\"\" \n",
    "    This function takes the df where the Cropland type for different classes is provided per location (row).\n",
    "    It adds all pixels per location; then is calculates the ratio of crop class in each location (% of total).\n",
    "    Finally is estimates the area per cropland type in each location by multiplying with the total area each row represents.\n",
    "    \n",
    "    INPUT: \n",
    "    df -> Pandas dataframe with LC type classification \n",
    "    col_list -> list of columns to include in the summary (e.g. LC0-LC1)\n",
    "    \n",
    "    OUTPUT: Updated dataframe with estimated area (sqkm) of cropland per row\n",
    "    \"\"\"\n",
    "    df[\"Crop_pix_sum\"] = df[col_list].sum(axis=1)\n",
    "    for col in col_list:\n",
    "        df[col] = df[col]/df[\"Crop_pix_sum\"]*df[\"Vor_area_ha\"]\n",
    "        \n",
    "    df = df.drop('Crop_pix_sum', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Elevation and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_elevation_and_slope(name, method, clusters, workspace,crs):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "\n",
    "    gdal.Warp(workspace + r\"\\dem.tif\",raster.name,dstSRS=crs)\n",
    "\n",
    "    def calculate_slope(DEM):\n",
    "        gdal.DEMProcessing(workspace + r'\\slope.tif', DEM, 'slope')\n",
    "        with rasterio.open(workspace + r'\\slope.tif') as dataset:\n",
    "            slope=dataset.read(1)\n",
    "        return slope\n",
    "\n",
    "    slope=calculate_slope(workspace + r\"\\dem.tif\")\n",
    "\n",
    "    slope = rasterio.open(workspace + r'\\slope.tif')\n",
    "    gdal.Warp(workspace + r'\\slope_4326.tif',slope.name,dstSRS='EPSG:4326')\n",
    "    slope_4326 = rasterio.open(workspace + r'\\slope_4326.tif')\n",
    "\n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        slope_4326.name,\n",
    "        stats=[\"majority\"],\n",
    "        prefix=\"sl_\", all_touched = True, geojson_out=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_elevation_and_slope_bulk(file_name, name, method, clusters, workspace,crs):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(file_name)\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "\n",
    "    gdal.Warp(workspace + r\"\\dem.tif\",raster.name,dstSRS=crs)\n",
    "\n",
    "    def calculate_slope(DEM):\n",
    "        gdal.DEMProcessing(workspace + r'\\slope.tif', DEM, 'slope')\n",
    "        with rasterio.open(workspace + r'\\slope.tif') as dataset:\n",
    "            slope=dataset.read(1)\n",
    "        return slope\n",
    "\n",
    "    slope=calculate_slope(workspace + r\"\\dem.tif\")\n",
    "\n",
    "    slope = rasterio.open(workspace + r'\\slope.tif')\n",
    "    gdal.Warp(workspace + r'\\slope_4326.tif',slope.name,dstSRS='EPSG:4326')\n",
    "    slope_4326 = rasterio.open(workspace + r'\\slope_4326.tif')\n",
    "\n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        slope_4326.name,\n",
    "        stats=[\"majority\"],\n",
    "        prefix=\"sl_\", all_touched = True, geojson_out=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def finalizing_rasters(workspace, clusters, crs):\n",
    "    output = workspace + r'\\placeholder.geojson'\n",
    "    with open(output, \"w\") as dst:\n",
    "        collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": list(clusters)}\n",
    "        dst.write(json.dumps(collection))\n",
    "  \n",
    "    clusters = gpd.read_file(output)\n",
    "    os.remove(output)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preparing_for_vectors(workspace, clusters, crs):   \n",
    "    clusters.crs = {'init' :'epsg:4326'}\n",
    "    clusters = clusters.to_crs({ 'init': crs}) \n",
    "    points = clusters.copy()\n",
    "    points[\"geometry\"] = points[\"geometry\"].centroid\n",
    "    points.to_file(workspace + r'\\clusters_cp.shp', driver='ESRI Shapefile')\n",
    "    print(datetime.datetime.now())    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preparing_for_vectors_updated(workspace, clusters, crs_proj):   \n",
    "    #clusters.crs = {'init' : crs}\n",
    "    cl_points = clusters.copy()\n",
    "    cl_points_proj = cl_points.to_crs({ 'init': crs_proj}) \n",
    "    cl_points_proj[\"geometry\"] = cl_points_proj[\"geometry\"].centroid\n",
    "    cl_points_proj.to_file(workspace + r'\\clusters_cp.shp', driver='ESRI Shapefile')\n",
    "    print(datetime.datetime.now())    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_lines(name, admin, crs, workspace, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    lines=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "\n",
    "    lines_clip = gpd.clip(lines, admin)\n",
    "    lines_clip.crs = {'init' :'epsg:4326'}\n",
    "    lines_proj=lines_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    lines_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    line = fiona.open(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    firstline = line.next()\n",
    "\n",
    "    schema = {'geometry' : 'Point', 'properties' : {'id' : 'int'},}\n",
    "    with fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\", \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "        for lines in line:\n",
    "            if lines[\"geometry\"] is not None:\n",
    "                first = shape(lines['geometry'])\n",
    "                length = first.length\n",
    "                for distance in range(0,int(length),100):\n",
    "                    point = first.interpolate(distance)\n",
    "                    output.write({'geometry' :mapping(point), 'properties' : {'id':1}})\n",
    "\n",
    "    lines_f = fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\")\n",
    "    lines = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in lines_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "\n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(lines, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_lines_bulk(file_name, name, admin, crs, workspace, clusters):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    lines=gpd.read_file(file_name)\n",
    "\n",
    "    lines_clip = gpd.clip(lines, admin)\n",
    "    lines_clip.crs = {'init' :'epsg:4326'}\n",
    "    lines_proj=lines_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    lines_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    line = fiona.open(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    firstline = line.next()\n",
    "\n",
    "    schema = {'geometry' : 'Point', 'properties' : {'id' : 'int'},}\n",
    "    with fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\", \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "        for lines in line:\n",
    "            if lines[\"geometry\"] is not None:\n",
    "                first = shape(lines['geometry'])\n",
    "                length = first.length\n",
    "                for distance in range(0,int(length),100):\n",
    "                    point = first.interpolate(distance)\n",
    "                    output.write({'geometry' :mapping(point), 'properties' : {'id':1}})\n",
    "\n",
    "    lines_f = fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\")\n",
    "    lines = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in lines_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "\n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(lines, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_shorelines(name, lines, crs, workspace, clusters):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    #lines=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "\n",
    "    #lines_clip = gpd.clip(lines, admin)\n",
    "    lines.crs = {'init' :'epsg:4326'}\n",
    "    lines_proj=lines.to_crs({ 'init': crs})\n",
    "\n",
    "    lines_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    line = fiona.open(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    firstline = line.next()\n",
    "\n",
    "    schema = {'geometry' : 'Point', 'properties' : {'id' : 'int'},}\n",
    "    with fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\", \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "        for lines in line:\n",
    "            if lines[\"geometry\"] is not None:\n",
    "                first = shape(lines['geometry'])\n",
    "                length = first.length\n",
    "                for distance in range(0,int(length),100):\n",
    "                    point = first.interpolate(distance)\n",
    "                    output.write({'geometry' :mapping(point), 'properties' : {'id':1}})\n",
    "\n",
    "    lines_f = fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\")\n",
    "    lines = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in lines_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "\n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(lines, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_points(name, admin, crs, workspace, clusters, mg_filter):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    points=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    if mg_filter:\n",
    "        points['umgid'] = range(0, len(points))\n",
    "        points_post = points\n",
    "\n",
    "    points_clip = gpd.clip(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    points_f = fiona.open(workspace + r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000.\n",
    "    if mg_filter:\n",
    "        z2 = results2.tolist()\n",
    "        clusters['umgid'] = z2\n",
    "\n",
    "    a = vector_overlap(points, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "    \n",
    "    if mg_filter:\n",
    "        clusters = pd.merge(clusters, points_post[['umgid', 'name', \"MV_network\", \"MG_type\"]], on='umgid', how = 'left')\n",
    "        clusters.rename(columns = {'name':'MGName',\n",
    "                                   'MV_network':'MGMVstatus',\n",
    "                                   'MG_type':'MGType'}, inplace = True)\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    if mg_filter:\n",
    "        del clusters['umgid']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_points_bulk(file_name, name, admin, crs, workspace, clusters, mg_filter):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    points=gpd.read_file(file_name)\n",
    "    if mg_filter:\n",
    "        points['umgid'] = range(0, len(points))\n",
    "        points_post = points\n",
    "\n",
    "    points_clip = gpd.clip(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    points_f = fiona.open(workspace + r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000.\n",
    "    if mg_filter:\n",
    "        z2 = results2.tolist()\n",
    "        clusters['umgid'] = z2\n",
    "\n",
    "    a = vector_overlap(points, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "    \n",
    "    if mg_filter:\n",
    "        clusters = pd.merge(clusters, points_post[['umgid', 'name', \"MV_network\", \"MG_type\"]], on='umgid', how = 'left')\n",
    "        clusters.rename(columns = {'name':'MGName',\n",
    "                                   'MV_network':'MGMVstatus',\n",
    "                                   'MG_type':'MGType'}, inplace = True)\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    if mg_filter:\n",
    "        del clusters['umgid']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing hydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_hydro(admin, crs, workspace, clusters, points, hydropowervalue, \n",
    "                     hydropowerunit):\n",
    "\n",
    "    points_clip = gpd.clip(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\HydropowerDist_proj.shp\", driver='ESRI Shapefile')\n",
    "    points_f = fiona.open(workspace +  r\"\\HydropowerDist_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\HydropowerDist_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    mytree = scipy.spatial.cKDTree(s1_arr)\n",
    "    dist, indexes = mytree.query(s2_arr)\n",
    "            \n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    z1=dist.tolist()\n",
    "    z2=indexes.tolist()\n",
    "    clusters['HydropowerDist'] = z1\n",
    "    clusters['HydropowerDist'] = clusters['HydropowerDist']/1000\n",
    "    clusters['HydropowerFID'] = z2\n",
    "    \n",
    "    z3 = []\n",
    "    for s in indexes:\n",
    "        z3.append(points[hydropowervalue][s])\n",
    "        \n",
    "    clusters['Hydropower'] = z3\n",
    "    \n",
    "    x = hydropowerunit\n",
    "    \n",
    "    if x is 'MW':\n",
    "        clusters['Hydropower'] = clusters['Hydropower']*1000\n",
    "    elif x is 'kW':\n",
    "        clusters['Hydropower'] = clusters['Hydropower']\n",
    "    else:\n",
    "        clusters['Hydropower'] = clusters['Hydropower']/1000\n",
    "\n",
    "    a = vector_overlap(points, clusters, 'HydropowerDist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id','HydropowerDist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters['HydropowerDist2'] == 0, 'HydropowerDist'] = 0\n",
    "\n",
    "    del clusters['HydropowerDist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Voronoi polygons for polygon settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def createVoronoi(admin, settlements, crs_projected, crs):\n",
    "    ##=================================================##\n",
    "    ## Generating boundaries based on the admin unit\n",
    "    ##=================================================##\n",
    "    #Create a large rectangle surrounding the admin boundaries\n",
    "    admin_gdf_buf_prj = admin.to_crs(crs_projected)\n",
    "    bound = admin_gdf_buf_prj.geometry[admin_gdf_buf.geometry.index[0]].buffer(50000).envelope.boundary \n",
    "\n",
    "    ##Create many points along the rectangle boundary. I create one every 100 m.\n",
    "    boundarypoints = [bound.interpolate(distance=d) for d in range(0, np.ceil(bound.length).astype(int), 100)]\n",
    "    boundarycoords = np.array([[p.x, p.y] for p in boundarypoints])\n",
    "    \n",
    "    print(\"Boundary area defined..\")\n",
    "    \n",
    "    ##===============================================================================================##\n",
    "    ## Get all points from polygon perimeter (excluding interior points in case of complex geometries)\n",
    "    ##===============================================================================================##\n",
    "    # Create an empty GeoDataFrame to store the points\n",
    "    points_df = gpd.GeoDataFrame(columns=['id', 'uid' 'geometry'])\n",
    "    #Project settlement layers\n",
    "    settles_gdf_prj = settlements.to_crs(crs_projected)\n",
    "\n",
    "    # Iterate over each row in the GeoDataFrame\n",
    "    for index, row in settles_gdf_prj.iterrows():\n",
    "        polygon_id = index  \n",
    "        polygon_uid = row['id']  # Assuming the index is the unique identifier for each polygon\n",
    "        geometry = row['geometry']\n",
    "    \n",
    "        # Check if the geometry is a Polygon or MultiPolygon\n",
    "        if geometry.geom_type == 'Polygon':\n",
    "            settles_gdf_prj_to_iterate = [geometry]\n",
    "        elif geometry.geom_type == 'MultiPolygon':\n",
    "            settles_gdf_prj_to_iterate = geometry.geoms\n",
    "    \n",
    "        # Iterate over each polygon (in case of MultiPolygon)\n",
    "        for polygon in settles_gdf_prj_to_iterate:\n",
    "            # Extract the exterior coordinates of the polygon\n",
    "            exterior_coords = list(polygon.exterior.coords)\n",
    "        \n",
    "            # Iterate over each vertex in the exterior\n",
    "            for coord in exterior_coords:\n",
    "                point = Point(coord)\n",
    "                # Append a row to the points DataFrame with the point and its corresponding polygon id\n",
    "                points_df = points_df.append({'id': polygon_id, 'uid': polygon_uid, 'geometry': point}, ignore_index=True)\n",
    "\n",
    "    # Convert the geometry column to the appropriate GeoSeries\n",
    "    points_df['geometry'] = gpd.GeoSeries(points_df['geometry'])\n",
    "    points_df = points_df.drop([\"uidgeometry\"], axis=1)\n",
    "    points_df.crs = crs_projected\n",
    "\n",
    "    print(\"Perimeter vertices generated..\")\n",
    "    \n",
    "    ##===============================================================================##\n",
    "    ## Generating the Voronoi polygons associated with all points & clip to boundaries\n",
    "    ##===============================================================================##\n",
    "    x = points_df.geometry.x.values\n",
    "    y = points_df.geometry.y.values\n",
    "    coords = np.vstack((x, y)).T\n",
    "\n",
    "    all_coords = np.concatenate((boundarycoords, coords)) #Create an array of all points on the boundary and inside the polygon\n",
    "\n",
    "    vor = Voronoi(points=all_coords)\n",
    "    lines = [shapely.geometry.LineString(vor.vertices[line]) for line in vor.ridge_vertices if -1 not in line]\n",
    "\n",
    "    polys = shapely.ops.polygonize(lines)\n",
    "    voronois = gpd.GeoDataFrame(geometry=gpd.GeoSeries(polys), crs=crs_proj)\n",
    "\n",
    "    polydf = gpd.GeoDataFrame(geometry=[admin_gdf_buf_prj.geometry[admin_gdf_buf.geometry.index[0]]], crs=crs_projected)\n",
    "    points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x=coords[:,0], y=coords[:,1], crs=crs_projected))\n",
    "\n",
    "    result = gpd.overlay(df1=voronois, df2=polydf, how=\"intersection\")\n",
    "\n",
    "    # Adding an index column\n",
    "    result['uniqueID'] = range(1, len(result)+1)\n",
    "    \n",
    "    print(\"Voronoi polygons generated..\")\n",
    "\n",
    "    ##==============================================================##\n",
    "    ##Getting all IDs of points within the same polygon to voronoi\n",
    "    ##==============================================================##\n",
    "\n",
    "    ## Adding a small buffed to get points within the voronoi\n",
    "    buffered_result = result.copy()\n",
    "    buffered_result['geometry'] = buffered_result.buffer(15)\n",
    "\n",
    "    ## Get Polygon ID to Voronoi with spatial join\n",
    "    buffered_result_withID = gpd.sjoin(buffered_result, points_df[[\"geometry\", \"uid\"]], \n",
    "                                       how='left').drop(['index_right'], axis=1)\n",
    "\n",
    "    ## Adding uid to the voronoi results\n",
    "    result = result.merge(buffered_result_withID[[\"uniqueID\", \"uid\"]], how=\"left\", on='uniqueID')\n",
    "\n",
    "    ## Dissolve results based to the \n",
    "    result_dissolved = result.dissolve(by='uid')\n",
    "    \n",
    "    print(\"Dissolved Voronoi polygons completed..\")\n",
    "\n",
    "    ###==============================================================##\n",
    "    ### Visualization for testing\n",
    "    ###==============================================================##\n",
    "    print ('Vizualizing...')\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    polydf.boundary.plot(ax=ax, edgecolor=\"blue\", linewidth=6)\n",
    "    #voronois.plot(ax=ax, color=\"red\", alpha=0.3, edgecolor=\"black\")\n",
    "    result_dissolved.plot(ax=ax, color=\"red\", alpha=0.3, edgecolor=\"black\")\n",
    "    admin_gdf_buf_prj.plot(ax=ax, color=\"green\", alpha=0.3, edgecolor=\"black\")\n",
    "    settles_gdf_prj.plot(ax=ax, color=\"maroon\")\n",
    "\n",
    "    ##==============================================================##\n",
    "    ## Estimating area\n",
    "    ##==============================================================##\n",
    "    #add area of each polygon\n",
    "    result_dissolved[\"Vor_area_sq.km\"] = result_dissolved.geometry.area/10**6\n",
    "    result_dissolved[\"Vor_area_ha\"] = result_dissolved[\"Vor_area_sq.km\"]*10**2\n",
    "    \n",
    "    #Revert to original crs\n",
    "    result_dissolved = result_dissolved.to_crs(crs)\n",
    "\n",
    "    return result_dissolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Based on centroids -- not used\n",
    "def createVoronoi_old(admin, settlements, crs):\n",
    "    #Create a large rectangle surrounding the admin boundaries\n",
    "    admin_gdf_buf_prj = admin_gdf_buf.to_crs(crs_proj)\n",
    "    bound = admin_gdf_buf_prj.geometry[admin_gdf_buf.geometry.index[0]].buffer(50000).envelope.boundary \n",
    "\n",
    "    ##Create many points along the rectangle boundary. I create one every 100 m.\n",
    "    boundarypoints = [bound.interpolate(distance=d) for d in range(0, np.ceil(bound.length).astype(int), 100)]\n",
    "    boundarycoords = np.array([[p.x, p.y] for p in boundarypoints])\n",
    "\n",
    "    #Get the points inside the polygon\n",
    "    settles_gdf_prj = settles_gdf.to_crs(crs_proj)\n",
    "    x = settles_gdf_prj.centroid.geometry.x.values\n",
    "    y = settles_gdf_prj.centroid.geometry.y.values\n",
    "    coords = np.vstack((x, y)).T\n",
    "\n",
    "    all_coords = np.concatenate((boundarycoords, coords)) #Create an array of all points on the boundary and inside the polygon\n",
    "\n",
    "    vor = Voronoi(points=all_coords)\n",
    "    lines = [shapely.geometry.LineString(vor.vertices[line]) for line in \n",
    "             vor.ridge_vertices if -1 not in line]\n",
    "\n",
    "    polys = shapely.ops.polygonize(lines)\n",
    "    voronois = gpd.GeoDataFrame(geometry=gpd.GeoSeries(polys), crs=crs_proj)\n",
    "\n",
    "    polydf = gpd.GeoDataFrame(geometry=[admin_gdf_buf_prj.geometry[admin_gdf_buf.geometry.index[0]]], crs=crs_proj)\n",
    "    points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x=coords[:,0], y=coords[:,1], crs=crs_proj))\n",
    "\n",
    "    result = gpd.overlay(df1=voronois, df2=polydf, how=\"intersection\")\n",
    "\n",
    "    settles_gdf_prj_cen = settles_gdf_prj.copy()\n",
    "    settles_gdf_prj_cen.geometry = settles_gdf_prj_cen.geometry.centroid\n",
    "    result = gpd.sjoin(result, settles_gdf_prj_cen[[\"geometry\", \"id\"]], how='left').drop(['index_right'], axis=1)\n",
    "\n",
    "    #fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    #polydf.boundary.plot(ax=ax, edgecolor=\"blue\", linewidth=6)\n",
    "    #voronois.plot(ax=ax, color=\"red\", alpha=0.3, edgecolor=\"black\")\n",
    "    #result.plot(ax=ax, color=\"red\", alpha=0.3, edgecolor=\"black\")\n",
    "    #admin_gdf_buf_prj.plot(ax=ax, color=\"green\", alpha=0.3, edgecolor=\"black\")\n",
    "    #points.plot(ax=ax, color=\"maroon\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the prioritization columns for filter visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_prio_columns(clusters):\n",
    "    if \"HF_kWh\" in clusters:\n",
    "        clusters[\"School\"] = np.where((clusters[\"HF_kWh\"] > 0), 1, 0)\n",
    "    \n",
    "    if \"EF_kWh\" in clusters:\n",
    "        clusters[\"Health_facility\"] = np.where((clusters[\"EF_kWh\"] > 0), 1, 0)\n",
    "    \n",
    "    if \"waterpoints_count\" in clusters:\n",
    "        clusters[\"Water_point\"] = np.where((clusters[\"waterpoints_count\"] > 0), 1, 0)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def cleaning_string_attributes(df, column_name):\n",
    "    df[column_name].replace(\"ï¿½\", 'u', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"-\", '_', regex=True, inplace=True)\n",
    "    df[column_name].replace(\" \", '_', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"/\", '_', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"'\", '_', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã©\", 'e', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã®\", 'i', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã¯\", 'i', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã´\", 'o', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã£\", 'a', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã§\", 'c', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã\", 'o', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã¨\", 'e', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã \", 'a', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ã¡\", 'a', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Ãº\", 'u', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"Âª\", 'a', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'NaN', regex=True, inplace=True) \n",
    "    df[column_name].fillna(value=np.nan, inplace=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def conditioning(clusters, workspace, popunit):\n",
    "    clusters = clusters.to_crs({ 'init': 'epsg:4326'}) \n",
    "\n",
    "    clusters = clusters.rename(columns={\"NightLight\": \"NightLights\", popunit : \"Pop\",})\n",
    "\n",
    "    if \"Area\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Area\": \"GridCellArea\"})\n",
    "        \n",
    "    if \"Pop_cellscount\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"ClusterCellscount\": \"ClusterCells\"})\n",
    "        \n",
    "    if \"ClusterCellscount\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CoreCellscount\": \"CoreCells\"})\n",
    "        \n",
    "    if \"CoreCellscount\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CoreCellscount\": \"CoreCells\"})\n",
    "        \n",
    "    if \"landcovermajority\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"landcovermajority\": \"LandCover\"})\n",
    "\n",
    "    if \"elevationmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"elevationmean\": \"Elevation\"})  \n",
    "\n",
    "    if \"sl_majority\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"sl_majority\": \"Slope\"})\n",
    "\n",
    "    if \"ghimean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"ghimean\": \"GHI\"})\n",
    "        \n",
    "    if \"traveltimemean\" in clusters:\n",
    "        clusters[\"traveltimemean\"] = clusters[\"traveltimemean\"]/60\n",
    "        clusters = clusters.rename(columns={\"traveltimemean\": \"TravelHours\"})\n",
    "    elif \"TravelHour\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"TravelHour\": \"TravelHours\"})\n",
    "        \n",
    "    if \"windmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"windmean\": \"WindVel\"})\n",
    "    \n",
    "    if \"Residentia\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Resudentia\": \"ResidentialDemandTierCustom\"})\n",
    "    elif \"customdemandmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"customdemandmean\": \"ResidentialDemandTierCustom\"})\n",
    "    else:\n",
    "        clusters[\"ResidentialDemandTierCustom\"] = 0\n",
    "        \n",
    "    if \"Urban_Demand_Indexmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Urban_Demand_Indexmean\": \"ResidentialDemandTierCustomUrban\"})\n",
    "    else:\n",
    "        clusters[\"ResidentialDemandTierCustomUrban\"] = 0\n",
    "        \n",
    "    if \"Rural_Demand_Indexmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Rural_Demand_Indexmean\": \"ResidentialDemandTierCustomRural\"})\n",
    "    else:\n",
    "        clusters[\"ResidentialDemandTierCustomRural\"] = 0\n",
    "    \n",
    "    if \"Substation\" in clusters:\n",
    "        clusers = clusters.rename(columns={\"Substation\": \"SubstationDist\"})\n",
    "    elif \"SubstationDist\" not in clusters:\n",
    "        clusters[\"SubstationDist\"] = 99999\n",
    "\n",
    "    if \"CurrentHVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CurrentHVL\": \"Existing_HVDist\"})\n",
    "    \n",
    "    if \"CurrentMVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CurrentMVL\": \"Existing_MVDist\"})\n",
    "    \n",
    "    if \"PlannedHVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"PlannedHVL\": \"Planned_HVDist\"})\n",
    "    \n",
    "    if \"PlannedMVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"PlannedMVL\": \"Planned_MVDist\"})\n",
    "\n",
    "    if \"Existing_HVDist\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Existing_HVDist\": \"CurrentHVLineDist\"})\n",
    "        if \"Planned_HVDist\" in clusters:    \n",
    "            mask = (clusters['Planned_HVDist'] > clusters['CurrentHVLineDist'])\n",
    "            clusters['Planned_HVDist'][mask] = clusters['CurrentHVLineDist']\n",
    "            clusters = clusters.rename(columns={\"Planned_HVDist\": \"PlannedHVLineDist\"})\n",
    "        else:\n",
    "            clusters[\"PlannedHVLineDist\"] = clusters[\"CurrentHVLineDist\"]\n",
    "    elif \"Existing_HVDist\" not in clusters and \"Planned_HVDist\" not in clusters:\n",
    "        clusters[\"PlannedHVLineDist\"] = 99999\n",
    "        clusters[\"CurrentHVLineDist\"] = 99999\n",
    "    else:\n",
    "        clusters[\"CurrentHVLineDist\"] = 99999\n",
    "        clusters = clusters.rename(columns={\"Planned_HVDist\": \"PlannedHVLineDist\"})\n",
    "\n",
    "    if \"Existing_MVDist\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Existing_MVDist\": \"CurrentMVLineDist\"})\n",
    "        if \"Planned_MVDist\" in clusters:    \n",
    "            mask = (clusters['Planned_MVDist'] > clusters['CurrentMVLineDist'])\n",
    "            clusters['Planned_MVDist'][mask] = clusters['CurrentMVLineDist']\n",
    "            clusters = clusters.rename(columns={\"Planned_MVDist\": \"PlannedMVLineDist\"})\n",
    "        else:\n",
    "            clusters[\"PlannedMVLineDist\"] = clusters[\"CurrentMVLineDist\"]\n",
    "    elif \"Existing_MVDist\" not in clusters and \"Planned_MVDist\" not in clusters:\n",
    "        clusters[\"PlannedMVLineDist\"] = 99999\n",
    "        clusters[\"CurrentMVLineDist\"] = 99999\n",
    "    else:\n",
    "        clusters[\"CurrentMVLineDist\"] = 99999\n",
    "        clusters = clusters.rename(columns={\"Planned_MVDist\": \"PlannedMVLineDist\"})\n",
    "\n",
    "    if \"RoadsDist\" not in clusters:\n",
    "        clusters = clusters.rename(columns={\"RoadsDist\": \"RoadDist\"})\n",
    "    else:\n",
    "        clusters[\"RoadDist\"] = 99999\n",
    "        \n",
    "    if \"Transforme\" in clusters: \n",
    "        clusters = clusters.rename(columns={\"Transforme\": \"TransformerDist\"})\n",
    "    elif \"TransformerDist\" not in clusters:\n",
    "        clusters[\"TransformerDist\"] = 99999\n",
    "\n",
    "    if \"Hydropower\" not in clusters:\n",
    "        clusters[\"Hydropower\"] = 0\n",
    "        \n",
    "    if \"Hydropow_1\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Hydropow_1\": \"HydropowerDist\"})\n",
    "    elif 'HydropowerDist' not in clusters:\n",
    "        clusters[\"HydropowerDist\"] = 99999\n",
    "        \n",
    "    if \"Hydropow_2\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Hydropow_2\": \"HydropowerFID\"})\n",
    "    elif \"HydropowerFID\" not in clusters:\n",
    "        clusters[\"HydropowerFID\"] = 0\n",
    "    \n",
    "    if \"IsUrban\" not in clusters:\n",
    "        clusters[\"IsUrban\"] = 0    \n",
    "        \n",
    "    if \"PerCapitaD\" not in clusters:\n",
    "        clusters[\"PerCapitaDemand\"] = 0\n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"PerCapitaD\": \"PerCapitaDemand\"})\n",
    "        \n",
    "    if \"HealthDema\" not in clusters:\n",
    "        clusters[\"HealthDemand\"] = 0     \n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"HealthDema\": \"HealthDemand\"})    \n",
    "    if \"HF_kWh\" in clusters:\n",
    "        clusters[\"HealthDemand\"] = clusters[\"HF_kWh\"]\n",
    "        \n",
    "    if \"EducationD\" not in clusters:\n",
    "        clusters[\"EducationDemand\"] = 0     \n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"EducationD\": \"EducationDemand\"})\n",
    "    if \"EF_kWh\" in clusters:\n",
    "        clusters[\"EducationDemand\"] = clusters[\"EF_kWh\"]\n",
    "        \n",
    "    if \"AgriDemand\" not in clusters:\n",
    "        clusters[\"AgriDemand\"] = 0  \n",
    "        \n",
    "    if \"Commercial\" not in clusters:\n",
    "        clusters[\"CommercialDemand\"] = 0\n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"Commercial\": \"CommercialDemand\"})\n",
    "        \n",
    "    if \"Conflict\" not in clusters:\n",
    "        clusters[\"Conflict\"] = 0       \n",
    "\n",
    "    if \"Electrific\" not in clusters:\n",
    "        clusters[\"ElectrificationOrder\"] = 0\n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"Electrific\": \"ElectrificationOrder\"})\n",
    "    \n",
    "    if \"Resident_1\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier1\"] = 7.74\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_1\": \"ResidentialDemandTier1\"})\n",
    "\n",
    "    if \"Resident_2\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier2\"] = 43.8\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_2\": \"ResidentialDemandTier2\"})\n",
    "\n",
    "    if \"Resident_3\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier3\"] = 160.6\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_3\": \"ResidentialDemandTier3\"})\n",
    "\n",
    "    if \"Resident_4\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier4\"] = 423.4\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_4\": \"ResidentialDemandTier4\"})\n",
    "    \n",
    "    if \"Resident_5\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier5\"] = 598.6\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_5\": \"ResidentialDemandTier5\"})\n",
    "        \n",
    "    if \"MGDist\" not in clusters:\n",
    "        clusters[\"MGDist\"] = 99999\n",
    "    \n",
    "    if \"MGName\" not in clusters:\n",
    "        clusters[\"MGName\"] = None\n",
    "        \n",
    "    if \"MGMVstatus\" not in clusters:\n",
    "        clusters[\"MGMVstatus\"] = None\n",
    "        \n",
    "    if \"MGType\" not in clusters:\n",
    "        clusters[\"MGType\"] = None\n",
    "        \n",
    "    if \"waterpoints_count\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"waterpoints_count\": \"waterpoints\"})\n",
    "    \n",
    "    clusters[\"X_deg\"] = clusters.geometry.centroid.x\n",
    "    \n",
    "    clusters[\"Y_deg\"] = clusters.geometry.centroid.y\n",
    "    \n",
    "    clusters[\"Commercial_Multiplier\"] = 0\n",
    "    \n",
    "    del clusters[\"geometry\"]\n",
    "    #clusters.to_file(workspace + r\"\\GEP-OnSSET_InputFile.shp\", driver='ESRI Shapefile')\n",
    "    clusters.to_csv(workspace + r\"\\GEP-OnSSET_InputFile.csv\", index=False)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    print(\"The extraction file is now ready for review & use in the workspace directory as 'GEP-OnSSET_InputFile.csv'!\")\n",
    "    \n",
    "    return clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
