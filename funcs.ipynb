{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "These are the functions needed in order to run the extraction notebook. **Please do not edit this file as it will break the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd   # Note that you require geopandas version >= 0.7 that incluse clip see here for installation (https://gis.stackexchange.com/questions/360127/geopandas-0-6-1-installed-instead-of-0-7-0-in-conda-windows-10#)\n",
    "import os\n",
    "import fiona\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "import rasterio.fill\n",
    "import shapely\n",
    "from shapely.geometry import shape, mapping\n",
    "import pyproj\n",
    "import json\n",
    "#from earthpy import clip    clip has been deprecated to geopandas\n",
    "#import earthpy.spatial as es\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "#import gdal\n",
    "from osgeo import gdal  #this solves the environment problem in some cases\n",
    "import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import scipy.spatial\n",
    "from scipy.spatial import Voronoi\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting admin 1 boundary name to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_admin1_name(clusters, admin_col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the admin 1 boundaries')\n",
    "    admin_1 = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    \n",
    "    # Apply spatial join \n",
    "    cluster_support_2 = gpd.sjoin(clusters_support, admin_1[[\"geometry\", admin_col_name]], op='intersects').drop(['index_right'], axis=1)\n",
    "    group_by_id = cluster_support_2.groupby([\"id\"]).sum().reset_index()\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', admin_col_name]], on='id', how = 'left')\n",
    "    clusters.rename(columns = {admin_col_name:'Admin_1'}, inplace = True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_admin1_name_bulk(clusters, file_name, admin_col_name, crs):\n",
    "    # Import layer\n",
    "    #messagebox.showinfo('OnSSET', 'Select the admin 1 boundaries')\n",
    "    admin_1 = gpd.read_file(file_name)\n",
    "    \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    \n",
    "    # Apply spatial join \n",
    "    cluster_support_2 = gpd.sjoin(clusters_support, admin_1[[\"geometry\", admin_col_name]], op='intersects').drop(['index_right'], axis=1)\n",
    "    group_by_id = cluster_support_2.groupby([\"id\"]).sum().reset_index()\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', admin_col_name]], on='id', how = 'left')\n",
    "    clusters.rename(columns = {admin_col_name:'Admin_1'}, inplace = True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_admin_name(clusters, admin, admin_col_name):\n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    clusters_support_centroid = clusters_support.copy()\n",
    "    clusters_support_centroid.geometry = clusters_support_centroid.centroid\n",
    "    \n",
    "    # Apply spatial join \n",
    "    clusters_support_centroid_2 = gpd.sjoin(clusters_support_centroid, admin[[\"geometry\", admin_col_name]], op='intersects').drop(['index_right'], axis=1)\n",
    "    group_by_id = clusters_support_centroid_2.groupby([\"id\"]).sum().reset_index()\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', admin_col_name]], on='id', how = 'left')\n",
    "    #clusters.rename(columns = {admin_col_name:'Admin_name'}, inplace = True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting IDP & Refugee camps characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_IDPs_RefugeeCamps_status(clusters, col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the layer of IDP')\n",
    "    idp_gdf = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    \n",
    "    # Apply spatial join and group by cluster \"id\"\n",
    "    pointsInPolygon = gpd.sjoin(idp_gdf, clusters_support, how=\"inner\", op='intersects')\n",
    "    pointsInPolygon[col_name]=1\n",
    "    group_by_id = pointsInPolygon.groupby([\"id\", col_name]).sum().reset_index().drop(\"index_right\", axis=1)\n",
    "    \n",
    "    # Merge back to clusters\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', col_name]], on='id', how = 'left')\n",
    "    \n",
    "    clusters[col_name] = np.where(clusters[col_name] > 0, 1, 0)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting No of building per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_buildings_in_clusters(clusters, col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the layer of building footprints')\n",
    "    gdf = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    #Converting polygon buildings to points\n",
    "    gdf_centroids = gpd.GeoDataFrame(gdf,\n",
    "                                     crs=\"EPSG:4326\",\n",
    "                                     geometry=[Point(xy) for xy in zip(gdf.centroid.x, gdf.centroid.y)])\n",
    "    \n",
    "    # Reverting clusters to original crs \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    #clusters_support.id = clusters_support.id.astype(int)\n",
    "    \n",
    "    # Apply spatial join and group by cluster \"id\"\n",
    "    pointsInPolygon = gpd.sjoin(gdf_centroids, clusters_support, how=\"inner\", op='intersects')\n",
    "    pointsInPolygon[col_name]=1\n",
    "    group_by_id = pointsInPolygon.groupby([\"id\"]).sum().reset_index().drop(\"index_right\", axis=1)\n",
    "    \n",
    "    # Merge back to clusters\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', col_name]], on='id', how = 'left')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    clusters[col_name] = clusters[col_name].fillna(0)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting No of water points per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_waterpoints_in_clusters(clusters, col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the layer of water points')\n",
    "    gdf = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    # Reverting clusters to original crs \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    \n",
    "    # Apply spatial join and group by cluster \"id\"\n",
    "    pointsInPolygon = gpd.sjoin(gdf, clusters_support, how=\"inner\", op='intersects')\n",
    "    pointsInPolygon[col_name]=1\n",
    "    group_by_id = pointsInPolygon.groupby([\"id\"]).sum().reset_index().drop(\"index_right\", axis=1)\n",
    "    \n",
    "    # Merge back to clusters\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', col_name]], on='id', how = 'left')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    clusters[col_name] = clusters[col_name].fillna(0)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_raster(name, method, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_raster_crop(name, prefix, method, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name+prefix, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_raster_bulk(file_name, name, method, clusters):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(file_name)\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "## Processing Categorical/Discrete Rasters\n",
    "def processing_raster_cat(path, raster, prefix, polys):\n",
    "    \"\"\"\n",
    "    This function calculates stats for categorical rasters and attributes them to the given vector features. \n",
    "    \n",
    "    INPUT: \n",
    "    path: the directory where the raster layer is stored \n",
    "    raster: the name and extention of the raster layer \n",
    "    prefix: string used as prefix when assigning features to the vectors\n",
    "    clusters: the vector layer containing the clusters\n",
    "    \n",
    "    OUTPUT:\n",
    "    geojson file of the vector features including the new attributes\n",
    "    \"\"\"    \n",
    "    raster=rasterio.open(path + '\\\\' + raster)\n",
    "    \n",
    "    polys = zonal_stats(\n",
    "        polys,\n",
    "        raster.name,\n",
    "        categorical=True,\n",
    "        prefix=prefix, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(\"{} processing completed at\".format(prefix), datetime.datetime.now())\n",
    "    return polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Land cover area estimator\n",
    "def calc_Crop_sqkm(df, col_list):\n",
    "    \"\"\" \n",
    "    This function takes the df where the Cropland type for different classes is provided per location (row).\n",
    "    It adds all pixels per location; then is calculates the ratio of crop class in each location (% of total).\n",
    "    Finally is estimates the area per cropland type in each location by multiplying with the total area each row represents.\n",
    "    \n",
    "    INPUT: \n",
    "    df -> Pandas dataframe with LC type classification \n",
    "    col_list -> list of columns to include in the summary (e.g. LC0-LC1)\n",
    "    \n",
    "    OUTPUT: Updated dataframe with estimated area (sqkm) of cropland per row\n",
    "    \"\"\"\n",
    "    df[\"Crop_pix_sum\"] = df[col_list].sum(axis=1)\n",
    "    for col in col_list:\n",
    "        df[col] = df[col]/df[\"Crop_pix_sum\"]*df[\"Vor_area_ha\"]\n",
    "        \n",
    "    df = df.drop('Crop_pix_sum', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population growth estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def estimate_pop_growth(gdf, urban_growth_rate, rural_growth_rate, base_year, target_year, \n",
    "                        status_col = \"dou_level1\", pop_col = \"pop_start_year\"):\n",
    "    \"\"\"\n",
    "    Estimates future population for each settlement in a GeoDataFrame based on growth rates and urban/rural status.\n",
    "\n",
    "    Parameters:\n",
    "    - gdf: GeoDataFrame with columns ['population', 'status', 'year']\n",
    "    - urban_growth_rate: Annual growth rate for urban areas (e.g., 0.02 for 2%)\n",
    "    - rural_growth_rate: Annual growth rate for rural areas (e.g., 0.01 for 1%)\n",
    "    - base_year: The year the raster data of existing population used in this analysis represent\n",
    "    - target_year: The year to which population is projected\n",
    "    - status_col: The name of the column in your dataframe that indicate urbanization status\n",
    "    - pop_col: The name of the column in your dataframe that indicate base year population\n",
    "\n",
    "    Returns:\n",
    "    - Updated GeoDataFrame with a new column 'estimated_population'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate number of years to project\n",
    "    gdf = gdf.copy()\n",
    "    gdf['years_diff'] = target_year - base_year\n",
    "    \n",
    "    # Determine growth rate based on status\n",
    "    gdf['growth_rate'] = np.where(\n",
    "        gdf[status_col].str.lower() == 'Rural',\n",
    "        rural_growth_rate,\n",
    "        urban_growth_rate\n",
    "    )\n",
    "    \n",
    "    # Estimate future population using exponential growth formula\n",
    "    gdf['pop_target_year'] = gdf[pop_col] * (1 + gdf['growth_rate']) ** gdf['years_diff']\n",
    "    \n",
    "    # Clean up helper columns if needed\n",
    "    gdf.drop(columns=['years_diff', 'growth_rate'], inplace=True)\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## No of HH estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def estimate_HH_size(gdf, pop_col, urban_HH_size=5, rural_HH_size=5, status_col=\"dou_level1\"):\n",
    "    \"\"\"\n",
    "    Estimates No of HH for each settlement in a GeoDataFrame based on urban/rural status.\n",
    "\n",
    "    Parameters:\n",
    "    - gdf: GeoDataFrame with columns ['pop', 'status']\n",
    "    - pop_col: The name of the column in your dataframe that indicate population\n",
    "    - urban_HH_size: Average HH size in urban areas (e.g., 5 ppl/HH)\n",
    "    - rural_HH_size: Average HH size in rural areas (e.g., 5 ppl/HH)\n",
    "    - status_col: The name of the column in your dataframe that indicate urbanization status\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - Updated GeoDataFrame with a new column 'HH'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate number of years to project\n",
    "    gdf = gdf.copy()\n",
    "    \n",
    "    # Determine growth rate based on status\n",
    "    gdf['HH_size'] = np.where(\n",
    "        gdf[status_col].str.lower() == 'Rural',\n",
    "        rural_HH_size,\n",
    "        urban_HH_size\n",
    "    )\n",
    "    \n",
    "    name = \"HH\" + \"_\" + pop_col[4:]\n",
    "    # Estimate future population using exponential growth formula\n",
    "    gdf[name] = gdf[pop_col] / gdf['HH_size']\n",
    "    \n",
    "    # Clean up helper columns if needed\n",
    "    gdf.drop(columns=['HH_size'], inplace=True)\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Elevation and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_elevation_and_slope(name, method, clusters, workspace,crs):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "\n",
    "    gdal.Warp(workspace + r\"\\dem.tif\",raster.name,dstSRS=crs)\n",
    "\n",
    "    def calculate_slope(DEM):\n",
    "        gdal.DEMProcessing(workspace + r'\\slope.tif', DEM, 'slope')\n",
    "        with rasterio.open(workspace + r'\\slope.tif') as dataset:\n",
    "            slope=dataset.read(1)\n",
    "        return slope\n",
    "\n",
    "    slope=calculate_slope(workspace + r\"\\dem.tif\")\n",
    "\n",
    "    slope = rasterio.open(workspace + r'\\slope.tif')\n",
    "    gdal.Warp(workspace + r'\\slope_4326.tif',slope.name,dstSRS='EPSG:4326')\n",
    "    slope_4326 = rasterio.open(workspace + r'\\slope_4326.tif')\n",
    "\n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        slope_4326.name,\n",
    "        stats=[\"majority\"],\n",
    "        prefix=\"sl_\", all_touched = True, geojson_out=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_elevation_and_slope_bulk(file_name, name, method, clusters, workspace,crs):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(file_name)\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "\n",
    "    gdal.Warp(workspace + r\"\\dem.tif\",raster.name,dstSRS=crs)\n",
    "\n",
    "    def calculate_slope(DEM):\n",
    "        gdal.DEMProcessing(workspace + r'\\slope.tif', DEM, 'slope')\n",
    "        with rasterio.open(workspace + r'\\slope.tif') as dataset:\n",
    "            slope=dataset.read(1)\n",
    "        return slope\n",
    "\n",
    "    slope=calculate_slope(workspace + r\"\\dem.tif\")\n",
    "\n",
    "    slope = rasterio.open(workspace + r'\\slope.tif')\n",
    "    gdal.Warp(workspace + r'\\slope_4326.tif',slope.name,dstSRS='EPSG:4326')\n",
    "    slope_4326 = rasterio.open(workspace + r'\\slope_4326.tif')\n",
    "\n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        slope_4326.name,\n",
    "        stats=[\"majority\"],\n",
    "        prefix=\"sl_\", all_touched = True, geojson_out=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def finalizing_rasters(workspace, clusters, crs):\n",
    "    output = workspace + r'\\placeholder.geojson'\n",
    "    with open(output, \"w\") as dst:\n",
    "        collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": list(clusters)}\n",
    "        dst.write(json.dumps(collection))\n",
    "  \n",
    "    clusters = gpd.read_file(output)\n",
    "    os.remove(output)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preparing_for_vectors(workspace, clusters, crs):   \n",
    "    clusters.crs = {'init' :'epsg:4326'}\n",
    "    clusters = clusters.to_crs({ 'init': crs}) \n",
    "    points = clusters.copy()\n",
    "    points[\"geometry\"] = points[\"geometry\"].centroid\n",
    "    points.to_file(workspace + r'\\clusters_cp.shp', driver='ESRI Shapefile')\n",
    "    print(datetime.datetime.now())    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preparing_for_vectors_updated(workspace, clusters, crs_proj):   \n",
    "    #clusters.crs = {'init' : crs}\n",
    "    cl_points = clusters.copy()\n",
    "    cl_points_proj = cl_points.to_crs({ 'init': crs_proj}) \n",
    "    cl_points_proj[\"geometry\"] = cl_points_proj[\"geometry\"].centroid\n",
    "    cl_points_proj.to_file(workspace + r'\\clusters_cp.shp', driver='ESRI Shapefile')\n",
    "    print(datetime.datetime.now())    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def processing_lines(name, admin, crs, workspace, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    lines=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "\n",
    "    lines_clip = gpd.clip(lines, admin)\n",
    "    lines_clip.crs = {'init' :'epsg:4326'}\n",
    "    lines_proj=lines_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    lines_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    line = fiona.open(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    firstline = line.next()\n",
    "\n",
    "    schema = {'geometry' : 'Point', 'properties' : {'id' : 'int'},}\n",
    "    with fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\", \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "        for lines in line:\n",
    "            if lines[\"geometry\"] is not None:\n",
    "                first = shape(lines['geometry'])\n",
    "                length = first.length\n",
    "                for distance in range(0,int(length),100):\n",
    "                    point = first.interpolate(distance)\n",
    "                    output.write({'geometry' :mapping(point), 'properties' : {'id':1}})\n",
    "\n",
    "    lines_f = fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\")\n",
    "    lines = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in lines_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "\n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(lines, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_lines_bulk(file_name, name, admin, crs, workspace, clusters):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    lines=gpd.read_file(file_name)\n",
    "\n",
    "    lines_clip = gpd.clip(lines, admin)\n",
    "    lines_clip.crs = {'init' :'epsg:4326'}\n",
    "    lines_proj=lines_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    lines_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    line = fiona.open(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    firstline = line.next()\n",
    "\n",
    "    schema = {'geometry' : 'Point', 'properties' : {'id' : 'int'},}\n",
    "    with fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\", \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "        for lines in line:\n",
    "            if lines[\"geometry\"] is not None:\n",
    "                first = shape(lines['geometry'])\n",
    "                length = first.length\n",
    "                for distance in range(0,int(length),100):\n",
    "                    point = first.interpolate(distance)\n",
    "                    output.write({'geometry' :mapping(point), 'properties' : {'id':1}})\n",
    "\n",
    "    lines_f = fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\")\n",
    "    lines = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in lines_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "\n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(lines, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_shorelines(name, lines, crs, workspace, clusters):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    #lines=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "\n",
    "    #lines_clip = gpd.clip(lines, admin)\n",
    "    lines.crs = {'init' :'epsg:4326'}\n",
    "    lines_proj=lines.to_crs({ 'init': crs})\n",
    "\n",
    "    lines_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    line = fiona.open(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    firstline = line.next()\n",
    "\n",
    "    schema = {'geometry' : 'Point', 'properties' : {'id' : 'int'},}\n",
    "    with fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\", \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "        for lines in line:\n",
    "            if lines[\"geometry\"] is not None:\n",
    "                first = shape(lines['geometry'])\n",
    "                length = first.length\n",
    "                for distance in range(0,int(length),100):\n",
    "                    point = first.interpolate(distance)\n",
    "                    output.write({'geometry' :mapping(point), 'properties' : {'id':1}})\n",
    "\n",
    "    lines_f = fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\")\n",
    "    lines = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in lines_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "\n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(lines, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_points(name, admin, crs, workspace, clusters, mg_filter):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    points=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    if mg_filter:\n",
    "        points['umgid'] = range(0, len(points))\n",
    "        points_post = points\n",
    "\n",
    "    points_clip = gpd.clip(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    points_f = fiona.open(workspace + r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000.\n",
    "    if mg_filter:\n",
    "        z2 = results2.tolist()\n",
    "        clusters['umgid'] = z2\n",
    "\n",
    "    a = vector_overlap(points, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "    \n",
    "    if mg_filter:\n",
    "        clusters = pd.merge(clusters, points_post[['umgid', 'name', \"MV_network\", \"MG_type\"]], on='umgid', how = 'left')\n",
    "        clusters.rename(columns = {'name':'MGName',\n",
    "                                   'MV_network':'MGMVstatus',\n",
    "                                   'MG_type':'MGType'}, inplace = True)\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    if mg_filter:\n",
    "        del clusters['umgid']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_points_bulk(file_name, name, admin, crs, workspace, clusters, mg_filter):\n",
    "    #messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    points=gpd.read_file(file_name)\n",
    "    if mg_filter:\n",
    "        points['umgid'] = range(0, len(points))\n",
    "        points_post = points\n",
    "\n",
    "    points_clip = gpd.clip(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    points_f = fiona.open(workspace + r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000.\n",
    "    if mg_filter:\n",
    "        z2 = results2.tolist()\n",
    "        clusters['umgid'] = z2\n",
    "\n",
    "    a = vector_overlap(points, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "    \n",
    "    if mg_filter:\n",
    "        clusters = pd.merge(clusters, points_post[['umgid', 'name', \"MV_network\", \"MG_type\"]], on='umgid', how = 'left')\n",
    "        clusters.rename(columns = {'name':'MGName',\n",
    "                                   'MV_network':'MGMVstatus',\n",
    "                                   'MG_type':'MGType'}, inplace = True)\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    if mg_filter:\n",
    "        del clusters['umgid']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing hydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_hydro(admin, crs, workspace, clusters, points, hydropowervalue, \n",
    "                     hydropowerunit):\n",
    "\n",
    "    points_clip = gpd.clip(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\HydropowerDist_proj.shp\", driver='ESRI Shapefile')\n",
    "    points_f = fiona.open(workspace +  r\"\\HydropowerDist_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\HydropowerDist_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    mytree = scipy.spatial.cKDTree(s1_arr)\n",
    "    dist, indexes = mytree.query(s2_arr)\n",
    "            \n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    z1=dist.tolist()\n",
    "    z2=indexes.tolist()\n",
    "    clusters['HydropowerDist'] = z1\n",
    "    clusters['HydropowerDist'] = clusters['HydropowerDist']/1000\n",
    "    clusters['HydropowerFID'] = z2\n",
    "    \n",
    "    z3 = []\n",
    "    for s in indexes:\n",
    "        z3.append(points[hydropowervalue][s])\n",
    "        \n",
    "    clusters['Hydropower'] = z3\n",
    "    \n",
    "    x = hydropowerunit\n",
    "    \n",
    "    if x is 'MW':\n",
    "        clusters['Hydropower'] = clusters['Hydropower']*1000\n",
    "    elif x is 'kW':\n",
    "        clusters['Hydropower'] = clusters['Hydropower']\n",
    "    else:\n",
    "        clusters['Hydropower'] = clusters['Hydropower']/1000\n",
    "\n",
    "    a = vector_overlap(points, clusters, 'HydropowerDist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id','HydropowerDist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters['HydropowerDist2'] == 0, 'HydropowerDist'] = 0\n",
    "\n",
    "    del clusters['HydropowerDist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Voronoi polygons for polygon settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def createVoronoi(admin, settlements, crs_projected, crs):\n",
    "    ##=================================================##\n",
    "    ## Generating boundaries based on the admin unit\n",
    "    ##=================================================##\n",
    "    #Create a large rectangle surrounding the admin boundaries\n",
    "    admin_gdf_buf_prj = admin.to_crs(crs_projected)\n",
    "    #bound = admin_gdf_buf_prj.geometry[admin_gdf_buf.geometry.index[0]].buffer(50000).envelope.boundary # Bug\n",
    "    bound = admin_gdf_buf_prj.geometry[admin_gdf_buf_prj.geometry.index[0]].buffer(50000).envelope.boundary\n",
    "    \n",
    "\n",
    "    ##Create many points along the rectangle boundary. I create one every 100 m.\n",
    "    boundarypoints = [bound.interpolate(distance=d) for d in range(0, np.ceil(bound.length).astype(int), 100)]\n",
    "    boundarycoords = np.array([[p.x, p.y] for p in boundarypoints])\n",
    "    \n",
    "    print(\"Boundary area defined..\")\n",
    "    \n",
    "    ##===============================================================================================##\n",
    "    ## Get all points from polygon perimeter (excluding interior points in case of complex geometries)\n",
    "    ##===============================================================================================##\n",
    "    # Create an empty GeoDataFrame to store the points\n",
    "    points_df = gpd.GeoDataFrame(columns=['id', 'uid' 'geometry'])\n",
    "    #Project settlement layers\n",
    "    settles_gdf_prj = settlements.to_crs(crs_projected)\n",
    "\n",
    "    # Iterate over each row in the GeoDataFrame\n",
    "    for index, row in settles_gdf_prj.iterrows():\n",
    "        polygon_id = index  \n",
    "        polygon_uid = row['id']  # Assuming the index is the unique identifier for each polygon\n",
    "        geometry = row['geometry']\n",
    "    \n",
    "        # Check if the geometry is a Polygon or MultiPolygon\n",
    "        if geometry.geom_type == 'Polygon':\n",
    "            settles_gdf_prj_to_iterate = [geometry]\n",
    "        elif geometry.geom_type == 'MultiPolygon':\n",
    "            settles_gdf_prj_to_iterate = geometry.geoms\n",
    "    \n",
    "        # Iterate over each polygon (in case of MultiPolygon)\n",
    "        for polygon in settles_gdf_prj_to_iterate:\n",
    "            # Extract the exterior coordinates of the polygon\n",
    "            exterior_coords = list(polygon.exterior.coords)\n",
    "        \n",
    "            # Iterate over each vertex in the exterior\n",
    "            for coord in exterior_coords:\n",
    "                point = Point(coord)\n",
    "                # Append a row to the points DataFrame with the point and its corresponding polygon id\n",
    "                points_df = points_df.append({'id': polygon_id, 'uid': polygon_uid, 'geometry': point}, ignore_index=True)\n",
    "\n",
    "    # Convert the geometry column to the appropriate GeoSeries\n",
    "    points_df['geometry'] = gpd.GeoSeries(points_df['geometry'])\n",
    "    points_df = points_df.drop([\"uidgeometry\"], axis=1)\n",
    "    points_df.crs = crs_projected\n",
    "\n",
    "    print(\"Perimeter vertices generated..\")\n",
    "    \n",
    "    ##===============================================================================##\n",
    "    ## Generating the Voronoi polygons associated with all points & clip to boundaries\n",
    "    ##===============================================================================##\n",
    "    x = points_df.geometry.x.values\n",
    "    y = points_df.geometry.y.values\n",
    "    coords = np.vstack((x, y)).T\n",
    "\n",
    "    all_coords = np.concatenate((boundarycoords, coords)) #Create an array of all points on the boundary and inside the polygon\n",
    "\n",
    "    vor = Voronoi(points=all_coords)\n",
    "    lines = [shapely.geometry.LineString(vor.vertices[line]) for line in vor.ridge_vertices if -1 not in line]\n",
    "\n",
    "    polys = shapely.ops.polygonize(lines)\n",
    "    # voronois = gpd.GeoDataFrame(geometry=gpd.GeoSeries(polys), crs=crs_proj) # Bug\n",
    "    voronois = gpd.GeoDataFrame(geometry=gpd.GeoSeries(polys), crs=crs_projected)\n",
    "\n",
    "    # polydf = gpd.GeoDataFrame(geometry=[admin_gdf_buf_prj.geometry[admin_gdf_buf.geometry.index[0]]], crs=crs_projected) # Bug\n",
    "    polydf = gpd.GeoDataFrame(geometry=[admin_gdf_buf_prj.geometry[admin_gdf_buf_prj.geometry.index[0]]], crs=crs_projected)\n",
    "    points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x=coords[:,0], y=coords[:,1], crs=crs_projected))\n",
    "\n",
    "    result = gpd.overlay(df1=voronois, df2=polydf, how=\"intersection\")\n",
    "\n",
    "    # Adding an index column\n",
    "    result['uniqueID'] = range(1, len(result)+1)\n",
    "    \n",
    "    print(\"Voronoi polygons generated..\")\n",
    "\n",
    "    ##==============================================================##\n",
    "    ##Getting all IDs of points within the same polygon to voronoi\n",
    "    ##==============================================================##\n",
    "\n",
    "    ## Adding a small buffed to get points within the voronoi\n",
    "    buffered_result = result.copy()\n",
    "    buffered_result['geometry'] = buffered_result.buffer(15)\n",
    "\n",
    "    ## Get Polygon ID to Voronoi with spatial join\n",
    "    buffered_result_withID = gpd.sjoin(buffered_result, points_df[[\"geometry\", \"uid\"]], \n",
    "                                       how='left').drop(['index_right'], axis=1)\n",
    "\n",
    "    ## Adding uid to the voronoi results\n",
    "    result = result.merge(buffered_result_withID[[\"uniqueID\", \"uid\"]], how=\"left\", on='uniqueID')\n",
    "\n",
    "    ## Dissolve results based to the \n",
    "    result_dissolved = result.dissolve(by='uid')\n",
    "    \n",
    "    print(\"Dissolved Voronoi polygons completed..\")\n",
    "\n",
    "    ##==============================================================##\n",
    "    ## Estimating area\n",
    "    ##==============================================================##\n",
    "    #add area of each polygon\n",
    "    result_dissolved[\"Vor_area_sq.km\"] = result_dissolved.geometry.area/10**6\n",
    "    result_dissolved[\"Vor_area_ha\"] = result_dissolved[\"Vor_area_sq.km\"]*10**2\n",
    "    \n",
    "    #Revert to original crs\n",
    "    result_dissolved = result_dissolved.to_crs(crs)\n",
    "\n",
    "    return result_dissolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Based on centroids -- not used\n",
    "def createVoronoi_old(admin, settlements, crs):\n",
    "    #Create a large rectangle surrounding the admin boundaries\n",
    "    admin_gdf_buf_prj = admin_gdf_buf.to_crs(crs_proj)\n",
    "    bound = admin_gdf_buf_prj.geometry[admin_gdf_buf.geometry.index[0]].buffer(50000).envelope.boundary \n",
    "\n",
    "    ##Create many points along the rectangle boundary. I create one every 100 m.\n",
    "    boundarypoints = [bound.interpolate(distance=d) for d in range(0, np.ceil(bound.length).astype(int), 100)]\n",
    "    boundarycoords = np.array([[p.x, p.y] for p in boundarypoints])\n",
    "\n",
    "    #Get the points inside the polygon\n",
    "    settles_gdf_prj = settles_gdf.to_crs(crs_proj)\n",
    "    x = settles_gdf_prj.centroid.geometry.x.values\n",
    "    y = settles_gdf_prj.centroid.geometry.y.values\n",
    "    coords = np.vstack((x, y)).T\n",
    "\n",
    "    all_coords = np.concatenate((boundarycoords, coords)) #Create an array of all points on the boundary and inside the polygon\n",
    "\n",
    "    vor = Voronoi(points=all_coords)\n",
    "    lines = [shapely.geometry.LineString(vor.vertices[line]) for line in \n",
    "             vor.ridge_vertices if -1 not in line]\n",
    "\n",
    "    polys = shapely.ops.polygonize(lines)\n",
    "    voronois = gpd.GeoDataFrame(geometry=gpd.GeoSeries(polys), crs=crs_proj)\n",
    "\n",
    "    polydf = gpd.GeoDataFrame(geometry=[admin_gdf_buf_prj.geometry[admin_gdf_buf.geometry.index[0]]], crs=crs_proj)\n",
    "    points = gpd.GeoDataFrame(geometry=gpd.points_from_xy(x=coords[:,0], y=coords[:,1], crs=crs_proj))\n",
    "\n",
    "    result = gpd.overlay(df1=voronois, df2=polydf, how=\"intersection\")\n",
    "\n",
    "    settles_gdf_prj_cen = settles_gdf_prj.copy()\n",
    "    settles_gdf_prj_cen.geometry = settles_gdf_prj_cen.geometry.centroid\n",
    "    result = gpd.sjoin(result, settles_gdf_prj_cen[[\"geometry\", \"id\"]], how='left').drop(['index_right'], axis=1)\n",
    "\n",
    "    #fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    #polydf.boundary.plot(ax=ax, edgecolor=\"blue\", linewidth=6)\n",
    "    #voronois.plot(ax=ax, color=\"red\", alpha=0.3, edgecolor=\"black\")\n",
    "    #result.plot(ax=ax, color=\"red\", alpha=0.3, edgecolor=\"black\")\n",
    "    #admin_gdf_buf_prj.plot(ax=ax, color=\"green\", alpha=0.3, edgecolor=\"black\")\n",
    "    #points.plot(ax=ax, color=\"maroon\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the prioritization columns for filter visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_prio_columns(clusters):\n",
    "    if \"HF_kWh\" in clusters:\n",
    "        clusters[\"School\"] = np.where((clusters[\"HF_kWh\"] > 0), 1, 0)\n",
    "    \n",
    "    if \"EF_kWh\" in clusters:\n",
    "        clusters[\"Health_facility\"] = np.where((clusters[\"EF_kWh\"] > 0), 1, 0)\n",
    "    \n",
    "    if \"waterpoints_count\" in clusters:\n",
    "        clusters[\"Water_point\"] = np.where((clusters[\"waterpoints_count\"] > 0), 1, 0)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def cleaning_string_attributes(df, column_name):\n",
    "    df[column_name].replace(\"\", 'u', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"-\", '_', regex=True, inplace=True)\n",
    "    df[column_name].replace(\" \", '_', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"/\", '_', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"'\", '_', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'e', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'i', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'i', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'o', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'a', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'c', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'o', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'e', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'a', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'a', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'u', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'a', regex=True, inplace=True)\n",
    "    df[column_name].replace(\"\", 'NaN', regex=True, inplace=True) \n",
    "    df[column_name].fillna(value=np.nan, inplace=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def conditioning(clusters, workspace, popunit):\n",
    "    clusters = clusters.to_crs({ 'init': 'epsg:4326'}) \n",
    "\n",
    "    clusters = clusters.rename(columns={\"NightLight\": \"NightLights\", popunit : \"Pop\",})\n",
    "\n",
    "    if \"Area\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Area\": \"GridCellArea\"})\n",
    "        \n",
    "    if \"Pop_cellscount\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"ClusterCellscount\": \"ClusterCells\"})\n",
    "        \n",
    "    if \"ClusterCellscount\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CoreCellscount\": \"CoreCells\"})\n",
    "        \n",
    "    if \"CoreCellscount\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CoreCellscount\": \"CoreCells\"})\n",
    "        \n",
    "    if \"landcovermajority\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"landcovermajority\": \"LandCover\"})\n",
    "\n",
    "    if \"elevationmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"elevationmean\": \"Elevation\"})  \n",
    "\n",
    "    if \"sl_majority\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"sl_majority\": \"Slope\"})\n",
    "\n",
    "    if \"ghimean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"ghimean\": \"GHI\"})\n",
    "        \n",
    "    if \"traveltimemean\" in clusters:\n",
    "        clusters[\"traveltimemean\"] = clusters[\"traveltimemean\"]/60\n",
    "        clusters = clusters.rename(columns={\"traveltimemean\": \"TravelHours\"})\n",
    "    elif \"TravelHour\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"TravelHour\": \"TravelHours\"})\n",
    "        \n",
    "    if \"windmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"windmean\": \"WindVel\"})\n",
    "    \n",
    "    if \"Residentia\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Resudentia\": \"ResidentialDemandTierCustom\"})\n",
    "    elif \"customdemandmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"customdemandmean\": \"ResidentialDemandTierCustom\"})\n",
    "    else:\n",
    "        clusters[\"ResidentialDemandTierCustom\"] = 0\n",
    "        \n",
    "    if \"Urban_Demand_Indexmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Urban_Demand_Indexmean\": \"ResidentialDemandTierCustomUrban\"})\n",
    "    else:\n",
    "        clusters[\"ResidentialDemandTierCustomUrban\"] = 0\n",
    "        \n",
    "    if \"Rural_Demand_Indexmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Rural_Demand_Indexmean\": \"ResidentialDemandTierCustomRural\"})\n",
    "    else:\n",
    "        clusters[\"ResidentialDemandTierCustomRural\"] = 0\n",
    "    \n",
    "    if \"Substation\" in clusters:\n",
    "        clusers = clusters.rename(columns={\"Substation\": \"SubstationDist\"})\n",
    "    elif \"SubstationDist\" not in clusters:\n",
    "        clusters[\"SubstationDist\"] = 99999\n",
    "\n",
    "    if \"CurrentHVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CurrentHVL\": \"Existing_HVDist\"})\n",
    "    \n",
    "    if \"CurrentMVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CurrentMVL\": \"Existing_MVDist\"})\n",
    "    \n",
    "    if \"PlannedHVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"PlannedHVL\": \"Planned_HVDist\"})\n",
    "    \n",
    "    if \"PlannedMVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"PlannedMVL\": \"Planned_MVDist\"})\n",
    "\n",
    "    if \"Existing_HVDist\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Existing_HVDist\": \"CurrentHVLineDist\"})\n",
    "        if \"Planned_HVDist\" in clusters:    \n",
    "            mask = (clusters['Planned_HVDist'] > clusters['CurrentHVLineDist'])\n",
    "            clusters['Planned_HVDist'][mask] = clusters['CurrentHVLineDist']\n",
    "            clusters = clusters.rename(columns={\"Planned_HVDist\": \"PlannedHVLineDist\"})\n",
    "        else:\n",
    "            clusters[\"PlannedHVLineDist\"] = clusters[\"CurrentHVLineDist\"]\n",
    "    elif \"Existing_HVDist\" not in clusters and \"Planned_HVDist\" not in clusters:\n",
    "        clusters[\"PlannedHVLineDist\"] = 99999\n",
    "        clusters[\"CurrentHVLineDist\"] = 99999\n",
    "    else:\n",
    "        clusters[\"CurrentHVLineDist\"] = 99999\n",
    "        clusters = clusters.rename(columns={\"Planned_HVDist\": \"PlannedHVLineDist\"})\n",
    "\n",
    "    if \"Existing_MVDist\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Existing_MVDist\": \"CurrentMVLineDist\"})\n",
    "        if \"Planned_MVDist\" in clusters:    \n",
    "            mask = (clusters['Planned_MVDist'] > clusters['CurrentMVLineDist'])\n",
    "            clusters['Planned_MVDist'][mask] = clusters['CurrentMVLineDist']\n",
    "            clusters = clusters.rename(columns={\"Planned_MVDist\": \"PlannedMVLineDist\"})\n",
    "        else:\n",
    "            clusters[\"PlannedMVLineDist\"] = clusters[\"CurrentMVLineDist\"]\n",
    "    elif \"Existing_MVDist\" not in clusters and \"Planned_MVDist\" not in clusters:\n",
    "        clusters[\"PlannedMVLineDist\"] = 99999\n",
    "        clusters[\"CurrentMVLineDist\"] = 99999\n",
    "    else:\n",
    "        clusters[\"CurrentMVLineDist\"] = 99999\n",
    "        clusters = clusters.rename(columns={\"Planned_MVDist\": \"PlannedMVLineDist\"})\n",
    "\n",
    "    if \"RoadsDist\" not in clusters:\n",
    "        clusters = clusters.rename(columns={\"RoadsDist\": \"RoadDist\"})\n",
    "    else:\n",
    "        clusters[\"RoadDist\"] = 99999\n",
    "        \n",
    "    if \"Transforme\" in clusters: \n",
    "        clusters = clusters.rename(columns={\"Transforme\": \"TransformerDist\"})\n",
    "    elif \"TransformerDist\" not in clusters:\n",
    "        clusters[\"TransformerDist\"] = 99999\n",
    "\n",
    "    if \"Hydropower\" not in clusters:\n",
    "        clusters[\"Hydropower\"] = 0\n",
    "        \n",
    "    if \"Hydropow_1\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Hydropow_1\": \"HydropowerDist\"})\n",
    "    elif 'HydropowerDist' not in clusters:\n",
    "        clusters[\"HydropowerDist\"] = 99999\n",
    "        \n",
    "    if \"Hydropow_2\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Hydropow_2\": \"HydropowerFID\"})\n",
    "    elif \"HydropowerFID\" not in clusters:\n",
    "        clusters[\"HydropowerFID\"] = 0\n",
    "    \n",
    "    if \"IsUrban\" not in clusters:\n",
    "        clusters[\"IsUrban\"] = 0    \n",
    "        \n",
    "    if \"PerCapitaD\" not in clusters:\n",
    "        clusters[\"PerCapitaDemand\"] = 0\n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"PerCapitaD\": \"PerCapitaDemand\"})\n",
    "        \n",
    "    if \"HealthDema\" not in clusters:\n",
    "        clusters[\"HealthDemand\"] = 0     \n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"HealthDema\": \"HealthDemand\"})    \n",
    "    if \"HF_kWh\" in clusters:\n",
    "        clusters[\"HealthDemand\"] = clusters[\"HF_kWh\"]\n",
    "        \n",
    "    if \"EducationD\" not in clusters:\n",
    "        clusters[\"EducationDemand\"] = 0     \n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"EducationD\": \"EducationDemand\"})\n",
    "    if \"EF_kWh\" in clusters:\n",
    "        clusters[\"EducationDemand\"] = clusters[\"EF_kWh\"]\n",
    "        \n",
    "    if \"AgriDemand\" not in clusters:\n",
    "        clusters[\"AgriDemand\"] = 0  \n",
    "        \n",
    "    if \"Commercial\" not in clusters:\n",
    "        clusters[\"CommercialDemand\"] = 0\n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"Commercial\": \"CommercialDemand\"})\n",
    "        \n",
    "    if \"Conflict\" not in clusters:\n",
    "        clusters[\"Conflict\"] = 0       \n",
    "\n",
    "    if \"Electrific\" not in clusters:\n",
    "        clusters[\"ElectrificationOrder\"] = 0\n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"Electrific\": \"ElectrificationOrder\"})\n",
    "    \n",
    "    if \"Resident_1\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier1\"] = 7.74\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_1\": \"ResidentialDemandTier1\"})\n",
    "\n",
    "    if \"Resident_2\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier2\"] = 43.8\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_2\": \"ResidentialDemandTier2\"})\n",
    "\n",
    "    if \"Resident_3\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier3\"] = 160.6\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_3\": \"ResidentialDemandTier3\"})\n",
    "\n",
    "    if \"Resident_4\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier4\"] = 423.4\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_4\": \"ResidentialDemandTier4\"})\n",
    "    \n",
    "    if \"Resident_5\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier5\"] = 598.6\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_5\": \"ResidentialDemandTier5\"})\n",
    "        \n",
    "    if \"MGDist\" not in clusters:\n",
    "        clusters[\"MGDist\"] = 99999\n",
    "    \n",
    "    if \"MGName\" not in clusters:\n",
    "        clusters[\"MGName\"] = None\n",
    "        \n",
    "    if \"MGMVstatus\" not in clusters:\n",
    "        clusters[\"MGMVstatus\"] = None\n",
    "        \n",
    "    if \"MGType\" not in clusters:\n",
    "        clusters[\"MGType\"] = None\n",
    "        \n",
    "    if \"waterpoints_count\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"waterpoints_count\": \"waterpoints\"})\n",
    "    \n",
    "    clusters[\"X_deg\"] = clusters.geometry.centroid.x\n",
    "    \n",
    "    clusters[\"Y_deg\"] = clusters.geometry.centroid.y\n",
    "    \n",
    "    clusters[\"Commercial_Multiplier\"] = 0\n",
    "    \n",
    "    del clusters[\"geometry\"]\n",
    "    #clusters.to_file(workspace + r\"\\GEP-OnSSET_InputFile.shp\", driver='ESRI Shapefile')\n",
    "    clusters.to_csv(workspace + r\"\\GEP-OnSSET_InputFile.csv\", index=False)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    print(\"The extraction file is now ready for review & use in the workspace directory as 'GEP-OnSSET_InputFile.csv'!\")\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds a min-max normalized column to a GeoDataFrame and returns the updated GeoDataFrame.\n",
    "def normalize_index_column(gdf, col_name, copy=False):\n",
    "    \"\"\"\n",
    "    Adds a min-max normalized column to a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame.\n",
    "        col_name (str): Column to normalize.\n",
    "        copy (bool, optional): If True, return a new GeoDataFrame with normalized column.\n",
    "                               If False (default), add normalized column to the original GeoDataFrame in place.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: GeoDataFrame with the normalized column added.\n",
    "    \"\"\"\n",
    "    min_val = gdf[col_name].min()\n",
    "    max_val = gdf[col_name].max()\n",
    "\n",
    "    if copy:\n",
    "        new_gdf = gdf.copy()\n",
    "        new_gdf[f\"{col_name}_norm\"] = (new_gdf[col_name] - min_val) / (max_val - min_val)\n",
    "        return new_gdf\n",
    "    else:\n",
    "        gdf[f\"{col_name}_norm\"] = (gdf[col_name] - min_val) / (max_val - min_val)\n",
    "        return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizes a column (series)\n",
    "def normalize_series(series):\n",
    "\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    \n",
    "    series_norm=(series - min_val)/(max_val - min_val)\n",
    "    \n",
    "    return series_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating post-harvest cooling demand indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Farming activity index (FAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_FAI(gdf, crop_ext_col, crop_int_col, crop_ext_w=0.5, crop_int_w=0.5):\n",
    "    \"\"\"\n",
    "    Create a Farming Activity Index (FAI) by combining the cropland extent within each \n",
    "    settlememt's influence area and the cropland intensity (percentage of influence area occupied by cropland)\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Input settlements GeoDataFrame\n",
    "        crop_ext_col (str): Column name of the settlements GeoDataFrame containing the cropland extent (area) within the influence area\n",
    "        crop_int_col (str): Column name of the settlements GeoDataFrame containing the cropland intensity (percentage of influence area occupied by cropland)\n",
    "        crop_ext_w (float, optional): Weight for the cropland extent. Must be between 0 and 1. Default is 0.5.\n",
    "        crop_int_w (float, optional): Weight for the cropland intensity. Must be between 0 and 1. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: A new GeoDataFrame with the 'FAI' column added, representing the weighted normalized farming activity index.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If weights are not between 0 and 1 or if their sum is not exactly 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate weights\n",
    "    if not (0 <= crop_ext_w <= 1 and 0 <= crop_int_w <= 1):\n",
    "        raise ValueError(\"Weights must be between 0 and 1.\")\n",
    "    if abs((crop_ext_w + crop_int_w) - 1) > 1e-9:\n",
    "        raise ValueError(\"The sum of weights must be exactly 1.\")\n",
    "    \n",
    "    # Copy GeoDataFrame\n",
    "    result = gdf\n",
    "    \n",
    "    # Normalize crop_ext_col\n",
    "    normalize_index_column(result, crop_ext_col, copy=False)\n",
    "    ext_norm_col = f\"{crop_ext_col}_norm\"\n",
    "    \n",
    "    # Normalize crop_int_col\n",
    "    normalize_index_column(result, crop_int_col, copy=False)\n",
    "    int_norm_col = f\"{crop_int_col}_norm\"\n",
    "    \n",
    "    # Compute weighted average of normalized columns\n",
    "    result['FAI'] = result[ext_norm_col] * crop_ext_w + result[int_norm_col] * crop_int_w\n",
    "    \n",
    "    \n",
    "    # Normalize FAI column\n",
    "    normalize_index_column(result, 'FAI', copy=False)\n",
    "    result['FAI_norm'] = result['FAI_norm'].round(3)\n",
    "    result['FAI'] = result['FAI'].round(3)\n",
    "    \n",
    "    # Drop intermediate normalized columns\n",
    "    result = result.drop(columns=[ext_norm_col, int_norm_col])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Accessibility\n",
    "Note: need to add weights validation and error if column does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_MAI(\n",
    "    gdf,\n",
    "    airport_dist_col,\n",
    "    port_dist_col,\n",
    "    railway_dist_col,\n",
    "    capital_dist_col,\n",
    "    cities_dist_col,\n",
    "    pop_20km_col,\n",
    "    airport_dist_w,\n",
    "    port_dist_w,\n",
    "    railway_dist_w,\n",
    "    capital_dist_w,\n",
    "    cities_dist_w,\n",
    "    pop_20km_w,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a normalized Market Accessibility Index (MAI) column as a weighted average of normalized accessibility indicators.\n",
    "    Returns only the normalized MAI column as a pandas Series.\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame containing settlement data.\n",
    "        airport_dist_col (str): Column name for travel time to closest airport (hours).\n",
    "        port_dist_col (str): Column name for travel time to closest main port (hours).\n",
    "        railway_dist_col (str): Column name for travel time to closest railway station (hours).\n",
    "        capital_dist_col (str): Column name for travel time to capital city (hours).\n",
    "        cities_dist_col (str): Column name for travel time to closest city with population > 200k (hours).\n",
    "        pop_20km_col (str): Column name for population count within 20km buffer.\n",
    "        airport_dist_w (float): Weight for airport travel time.\n",
    "        port_dist_w (float): Weight for port travel time.\n",
    "        railway_dist_w (float): Weight for railway travel time.\n",
    "        capital_dist_w (float): Weight for capital travel time.\n",
    "        cities_dist_w (float): Weight for city travel time.\n",
    "        pop_20km_w (float): Weight for population count.\n",
    "\n",
    "    Returns:\n",
    "        pandas.Series: Normalized Market Accessibility Index ('MAI_norm') values.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the sum of weights is not 1 or if any input column is missing from gdf.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate that all input columns exist in gdf\n",
    "    required_columns = [\n",
    "        airport_dist_col,\n",
    "        port_dist_col,\n",
    "        railway_dist_col,\n",
    "        capital_dist_col,\n",
    "        cities_dist_col,\n",
    "        pop_20km_col,\n",
    "    ]\n",
    "    missing_cols = [col for col in required_columns if col not in gdf.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"The following required columns are missing from the GeoDataFrame: {missing_cols}\")\n",
    "\n",
    "    # Validate that weights sum to 1 (allowing for floating point tolerance)\n",
    "    total_weight = (\n",
    "        airport_dist_w\n",
    "        + port_dist_w\n",
    "        + railway_dist_w\n",
    "        + capital_dist_w\n",
    "        + cities_dist_w\n",
    "        + pop_20km_w\n",
    "    )\n",
    "    if not abs(total_weight - 1.0) < 1e-8:\n",
    "        raise ValueError(f\"The sum of weights must be 1.0, but it is {total_weight}\")\n",
    "\n",
    "    travel_cols = [\n",
    "        airport_dist_col,\n",
    "        port_dist_col,\n",
    "        railway_dist_col,\n",
    "        capital_dist_col,\n",
    "        cities_dist_col,\n",
    "    ]\n",
    "\n",
    "    # Step 1: Normalize each travel time column using normalize_series\n",
    "    norm_travel_cols = []\n",
    "    for col in travel_cols:\n",
    "        norm_col = f\"{col}_norm\"\n",
    "        gdf[norm_col] = normalize_series(gdf[col])\n",
    "        norm_travel_cols.append(norm_col)\n",
    "\n",
    "    # Step 2: Convert normalized travel times to accessibility indicators: 1 - normalized travel time\n",
    "    for norm_col in norm_travel_cols:\n",
    "        gdf[f\"{norm_col}_acc\"] = 1 - gdf[norm_col]\n",
    "\n",
    "    # Step 3: Normalize population column\n",
    "    pop_norm_col = f\"{pop_20km_col}_norm\"\n",
    "    gdf[pop_norm_col] = normalize_series(gdf[pop_20km_col])\n",
    "\n",
    "    # Step 4: Calculate weighted average of accessibility indicators and normalized population\n",
    "    weighted_avg = (\n",
    "        gdf[f\"{airport_dist_col}_norm_acc\"] * airport_dist_w\n",
    "        + gdf[f\"{port_dist_col}_norm_acc\"] * port_dist_w\n",
    "        + gdf[f\"{railway_dist_col}_norm_acc\"] * railway_dist_w\n",
    "        + gdf[f\"{capital_dist_col}_norm_acc\"] * capital_dist_w\n",
    "        + gdf[f\"{cities_dist_col}_norm_acc\"] * cities_dist_w\n",
    "        + gdf[pop_norm_col] * pop_20km_w\n",
    "    ) / total_weight  # total_weight is 1, so division is optional but kept for clarity\n",
    "\n",
    "    # Step 5: Normalize the weighted average (MAI) using normalize_series\n",
    "    MAI_norm = normalize_series(weighted_avg).round(3)\n",
    "\n",
    "    # Step 6: Drop intermediate columns created during normalization\n",
    "    cols_to_drop = norm_travel_cols + [f\"{col}_acc\" for col in norm_travel_cols] + [pop_norm_col]\n",
    "    gdf.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    return MAI_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-harvest cooling demand index for export market (DI_COOL_PH_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DI_COOL_PH_exp(gdf, \n",
    "                         FAI_col, avg_temp_col, HD30_hist_col, MAI_PH_exp_col,\n",
    "                         FAI_w=0.5, avg_temp_w=0.1, HD30_hist_w=0.1, MAI_PH_exp_w=0.3):\n",
    "    \"\"\"\n",
    "    Create a Post-harvest cooling demand index for export market (DI_COOL_PH_exp) as a normalized weighted average of:\n",
    "    - normalized farming activity index (FAI),\n",
    "    - average air temperature (avg_temp),\n",
    "    - number of hot days (HD30_hist),\n",
    "    - normalized market accessibility indicator for post-harvest demand and export markets (MAI_PH_exp).\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame to be modified in place.\n",
    "        FAI_col (str): Column name for normalized farming activity index (FAI).\n",
    "        avg_temp_col (str): Column name for average air temperature (not yet normalized).\n",
    "        HD30_hist_col (str): Column name for number of hot days (not yet normalized).\n",
    "        MAI_PH_exp_col (str): Column name for normalized market accessibility indicator for post-harvest demand and export markets.\n",
    "        FAI_w (float, optional): Weight for FAI. Default is 0.5.\n",
    "        avg_temp_w (float, optional): Weight for avg_temp. Default is 0.1.\n",
    "        HD30_hist_w (float, optional): Weight for HD30_hist. Default is 0.1.\n",
    "        MAI_PH_exp_w (float, optional): Weight for MAI_PH_exp. Default is 0.3.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: The input GeoDataFrame with the 'DI_COOL_PH_exp' column added.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If weights are not between 0 and 1 or if their sum is not exactly 1.\n",
    "        KeyError: If any specified column is missing.\n",
    "    \"\"\"\n",
    "    # Validate weights\n",
    "    weights = [FAI_w, avg_temp_w, HD30_hist_w, MAI_PH_exp_w]\n",
    "    if any(w < 0 or w > 1 for w in weights):\n",
    "        raise ValueError(\"All weights must be between 0 and 1.\")\n",
    "    if abs(sum(weights) - 1) > 1e-9:\n",
    "        raise ValueError(\"The sum of weights must be exactly 1.\")\n",
    "    \n",
    "    # Validate columns exist\n",
    "    for col in [FAI_col, avg_temp_col, HD30_hist_col, MAI_PH_exp_col]:\n",
    "        if col not in gdf.columns:\n",
    "            raise KeyError(f\"Column '{col}' not found in GeoDataFrame.\")\n",
    "    \n",
    "    # Normalize avg_temp and HD30_hist into temporary columns (in place)\n",
    "    normalize_index_column(gdf, avg_temp_col, copy=False)\n",
    "    normalize_index_column(gdf, HD30_hist_col, copy=False)\n",
    "    avg_temp_norm_col = f\"{avg_temp_col}_norm\"\n",
    "    HD30_hist_norm_col = f\"{HD30_hist_col}_norm\"\n",
    "    \n",
    "    # Compute weighted average of normalized columns\n",
    "    gdf['DI_COOL_PH_exp_raw'] = (\n",
    "        gdf[FAI_col] * FAI_w +\n",
    "        gdf[avg_temp_norm_col] * avg_temp_w +\n",
    "        gdf[HD30_hist_norm_col] * HD30_hist_w +\n",
    "        gdf[MAI_PH_exp_col] * MAI_PH_exp_w\n",
    "    )\n",
    "    \n",
    "    # Normalize the weighted average column in place\n",
    "    normalize_index_column(gdf, 'DI_COOL_PH_exp_raw', copy=False)\n",
    "    \n",
    "    # Rename normalized column to final index name\n",
    "    gdf.rename(columns={'DI_COOL_PH_exp_raw_norm': 'DI_COOL_PH_exp'}, inplace=True)\n",
    "    \n",
    "    # Drop intermediate raw and temporary normalized columns\n",
    "    gdf.drop(columns=['DI_COOL_PH_exp_raw', avg_temp_norm_col, HD30_hist_norm_col], inplace=True)\n",
    "    \n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DI_COOL_PH(\n",
    "    gdf,\n",
    "    FAI_norm_col,\n",
    "    avg_temp_col,\n",
    "    HD30_hist_col,\n",
    "    MAI_PH_exp_col,\n",
    "    FAI_w=0.5,\n",
    "    avg_temp_w=0.1,\n",
    "    HD30_hist_w=0.1,\n",
    "    MAI_PH_exp_w=0.3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a Post-harvest cooling demand index for export market (DI_COOL_PH_exp) as a normalized weighted average of:\n",
    "    - normalized farming activity index (FAI_norm),\n",
    "    - average air temperature (avg_temp),\n",
    "    - number of hot days (HD30_hist),\n",
    "    - normalized market accessibility index for post-harvest demand and export markets (MAI_PH_exp).\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): Input GeoDataFrame.\n",
    "        FAI_norm_col (str): Column name for normalized farming activity index (FAI_norm).\n",
    "        avg_temp_col (str): Column name for average air temperature (not yet normalized).\n",
    "        HD30_hist_col (str): Column name for number of hot days (not yet normalized).\n",
    "        MAI_PH_exp_col (str): Column name for normalized market accessibility index for post-harvest demand and export markets.\n",
    "        FAI_w (float, optional): Weight for FAI. Default is 0.5.\n",
    "        avg_temp_w (float, optional): Weight for avg_temp. Default is 0.1.\n",
    "        HD30_hist_w (float, optional): Weight for HD30_hist. Default is 0.1.\n",
    "        MAI_PH_exp_w (float, optional): Weight for MAI_PH_exp. Default is 0.3.\n",
    "\n",
    "    Returns:\n",
    "        pandas.Series: Normalized DI_COOL_PH_exp values.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If weights are not between 0 and 1 or if their sum is not exactly 1.\n",
    "        KeyError: If any specified column is missing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate weights\n",
    "    weights = [FAI_w, avg_temp_w, HD30_hist_w, MAI_PH_exp_w]\n",
    "    if any(w < 0 or w > 1 for w in weights):\n",
    "        raise ValueError(\"All weights must be between 0 and 1.\")\n",
    "    if abs(sum(weights) - 1) > 1e-9:\n",
    "        raise ValueError(\"The sum of weights must be exactly 1.\")\n",
    "\n",
    "    # Validate columns exist\n",
    "    for col in [FAI_norm_col, avg_temp_col, HD30_hist_col, MAI_PH_exp_col]:\n",
    "        if col not in gdf.columns:\n",
    "            raise KeyError(f\"Column '{col}' not found in GeoDataFrame.\")\n",
    "\n",
    "    # Normalize avg_temp and HD30_hist\n",
    "    avg_temp_norm = normalize_series(gdf[avg_temp_col])\n",
    "    HD30_hist_norm = normalize_series(gdf[HD30_hist_col])\n",
    "\n",
    "    # Compute weighted average of normalized columns\n",
    "    weighted_raw = (\n",
    "        gdf[FAI_norm_col] * FAI_w +\n",
    "        avg_temp_norm * avg_temp_w +\n",
    "        HD30_hist_norm * HD30_hist_w +\n",
    "        gdf[MAI_PH_exp_col] * MAI_PH_exp_w\n",
    "    )\n",
    "\n",
    "    # Normalize the weighted average\n",
    "    DI_COOL_PH_exp_norm = normalize_series(weighted_raw)\n",
    "\n",
    "    return DI_COOL_PH_exp_norm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
